{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import *\n",
    "import statsmodels.formula.api as smf\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Input, layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D, LeakyReLU, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from fireTS.models import NARX\n",
    "seed_value = 2022\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "from sklearn.preprocessing import QuantileTransformer \n",
    "normalizer = QuantileTransformer(output_distribution='normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_data=utils.caricaLOB('bynd',2/6)\n",
    "ld=lob_data.values.tolist()\n",
    "of=utils.OF_1(ld)\n",
    "bCols=np.ravel([['aOF_%d'%level, 'bOF_%d'%level] for level in [1,2,3]])\n",
    "of_data=pd.DataFrame(of, columns=bCols)\n",
    "m=pd.DataFrame(utils.midPrice(lob_data),columns=['m'])\n",
    "m['r']=m['m'].pct_change().fillna(0).copy()#.apply(np.log)\n",
    "ret=utils.preparaRitorni(m,'bynd')\n",
    "#of_data[]=ret#(m['r'])#.cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  9., 13., 18., 23., 27., 32., 37., 41., 46.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=utils.deltaT(m['r'])\n",
    "hk=utils.doHk(dt,11)\n",
    "hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = pd.concat([of_data[-len(ret):], ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#of=of.apply(lambda x : utils.taglia_e_cuci(x), axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(602968, 100, 6) (602968, 10)\n",
      "(200923, 100, 6) (200923, 10)\n",
      "(200923, 100, 6) (200923, 10)\n"
     ]
    }
   ],
   "source": [
    "V = int(len(of)*0.6)\n",
    "T = int(len(of)*0.8)\n",
    "\n",
    "n_train     = of.iloc[ :V].copy()\n",
    "n_vali      = of.iloc[V:T].copy()\n",
    "n_test      = of.iloc[T: ].copy()\n",
    "\n",
    "n_train = n_train.apply(lambda x : utils.taglia_e_cuci(x), axis=1, raw=True)\n",
    "n_vali  = n_vali .apply(lambda x : utils.taglia_e_cuci(x), axis=1, raw=True)\n",
    "n_test  = n_test .apply(lambda x : utils.taglia_e_cuci(x), axis=1, raw=True)\n",
    "\n",
    "x_vars = of.columns#[:-10]\n",
    "\n",
    "# Normalize Features\n",
    "# Dividing by the standard deviation estimated using the training set\n",
    "for x_var in x_vars:\n",
    "    sd = n_train[x_var].std()\n",
    "    n_train[x_var] = n_train[x_var] / sd\n",
    "    n_vali [x_var] = n_vali [x_var] / sd\n",
    "    n_test [x_var] = n_test [x_var] / sd\n",
    "\n",
    "trainX,trainY = utils.prepXY  (n_train, typo = 'lstm')    \n",
    "valiX ,valiY  = utils.prepXY  (n_vali , typo = 'lstm')    \n",
    "testX ,testY  = utils.prepXY  (n_test , typo = 'lstm')   \n",
    "\n",
    "\n",
    "print(trainX.shape ,trainY.shape)\n",
    "print(valiX.shape  ,valiY .shape)\n",
    "print(testX.shape  ,testY .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(testY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architettura della rete\n",
    "num_units = 64\n",
    "activation_function = 'leaky_relu'\n",
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_function = 'mse'\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "i = Input(shape=(100, 6))\n",
    "x = LSTM(64, return_sequences=True)(i)\n",
    "x = keras.layers.LeakyReLU(alpha=0.01)(x)\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.01)(x)\n",
    "x = keras.layers.Dropout(0.7)(x, training=True)\n",
    "#x = LSTM(64, return_sequences=True)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(10, activation='leaky_relu')(x)\n",
    "\n",
    "\n",
    "model = Model(i, x)\n",
    "\n",
    "\n",
    "callback=tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")\n",
    "\n",
    "model.compile(optimizer = adam, loss = loss_function)\n",
    "\n",
    "#r=model.fit(trainX,trainY, batch_size = 256, epochs = num_epochs,validation_data=(valiX ,valiY ),callbacks=[callback])\n",
    "\n",
    "#model.save('LSTMbynd25pct.h5')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss=r.history['loss']\n",
    "#lossVal=r.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('LSTMbynd.h5')\n",
    "model.load_weights('LSTMbyndPit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(r.history['loss']    [1:], label='train loss')\n",
    "#plt.plot(r.history['val_loss'][1:], label='test loss')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18843/18843 [==============================] - 651s 34ms/step\n",
      "6279/6279 [==============================] - 231s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "Ptrain= model.predict(trainX)\n",
    "Ptest = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00057878, 0.00112619, 0.00172947, 0.0021896 , 0.00266304,\n",
       "       0.00317812, 0.00364115, 0.00408428, 0.00444944, 0.00435618])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2LSTMos = np.zeros(10)\n",
    "for i in range(10):\n",
    "    r2LSTMos[i]=1-mse(y_pred=Ptest[:,i],y_true=testY[:,i])/mse(y_pred=[np.mean(trainY[:,i])]*len(testY),y_true=testY[:,i])\n",
    "r2LSTMos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0005327 , 0.00100545, 0.00158612, 0.00198979, 0.00246686,\n",
       "       0.0028956 , 0.00333818, 0.00373555, 0.00402073, 0.00433913])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2LSTMis = np.zeros(10)\n",
    "for i in range(10):\n",
    "    r2LSTMis[i]=1-mse(y_pred=Ptrain[:,i],y_true=trainY[:,i])/mse(y_pred=[np.mean(trainY[:,i])]*len(trainY),y_true=trainY[:,i])\n",
    "r2LSTMis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0020024194770638325, 0.002591012553303773)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_pred=Ptest*sd,y_true=testY*sd), r2_score(y_pred=Ptrain*sd,y_true=trainY*sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(testY)\n",
    "#plt.plot(Ptest,label='pred')\n",
    "#plt.legend()\n",
    "#plt.plot()\n",
    "####\n",
    "#plt.plot(trainY)\n",
    "#plt.plot(Ptrain,label='pred')\n",
    "#plt.legend()\n",
    "#plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "597e2b791014bf3f975e304e1e72922cb6b9245e0453d34e2c818a7120fc527f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
